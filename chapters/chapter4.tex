\chapter{Heuristic and Metaheuristic Algorithms}\label{chapt:4}

\section{Heuristic Approaches}

Heuristic approaches to solving problems like the Travelling Salesman Problem (TSP) are strategies designed to find good-enough solutions within a reasonable amount of time, especially when an exact solution is not feasible due to the problem's complexity. These methods are essential in cases where the computational cost of finding the optimal solution is prohibitive, as is common with NP-hard problems such as the TSP.

\subsection{Concept and Necessity}
The concept of heuristic approaches is grounded in the idea of making educated guesses or following intuitive algorithms that aim towards finding solutions that are close to the best possible, without necessarily guaranteeing optimality. Heuristics are characterized by their rule-of-thumb strategies which make them significantly faster than exact methods in practice, albeit at the cost of potentially missing the optimal solution.

The necessity of heuristics stems from the computational intractability of many real-world problems. For the TSP, as the number of cities increases, the search space (i.e., the total number of possible tours) grows factorially, making it impossible to explore every option directly through brute-force methods in a reasonable time frame. Heuristics offer a pragmatic alternative by providing solutions that, while not guaranteed to be optimal, are sufficiently accurate for practical purposes and can be obtained much more quickly.


\subsection{Greedy Algorithms}

Greedy algorithms constitute a significant class of heuristic approaches characterized by their strategy of making the locally optimal choice at each stage with the hope of finding a global optimum. In the context of the TSP and similar optimization problems, greedy algorithms simplify decision-making processes by breaking down a problem into a series of steps and choosing the best option available at each step without considering the broader consequences of these choices.

\subsubsection{Definition and Principle}

A greedy algorithm builds up a solution piece by piece, always choosing the next piece that offers the most immediate benefit. This approach is simple and straightforward, making it an attractive option for many problems where a quick and easy solution is more valuable than an optimal one. The efficacy of greedy algorithms varies widely from one problem to another; in some cases, they achieve an optimal solution, while in others, they may only reach a suboptimal solution.

\subsubsection{Application to TSP: Nearest Neighbor Search}

In the TSP, a classic application of the greedy algorithm is the Nearest Neighbor Search, where the algorithm constructs a tour by starting from an arbitrary city and, at each step, extending the tour by moving to the nearest unvisited city until all cities are visited.

\paragraph{Pseudocode}

\begin{algorithm}
	\caption{Greedy Best First Search for TSP}\label{alg:greedybestfirst}
	\begin{algorithmic}[1]
		\Procedure{NearestNeighborSearch}{$cities, distances$}
		\State $start \gets$ select an arbitrary city from $cities$
		\State $current \gets start$
		\State $tour \gets$ list containing $start$
		\State $unvisited \gets cities \setminus \{start\}$
		\While{$unvisited \neq \emptyset$}
		\State $next \gets$ city in $unvisited$ with minimum distance from $current$
		\State add $next$ to $tour$
		\State remove $next$ from $unvisited$
		\State $current \gets next$
		\EndWhile
		\State add $start$ to $tour$ to close the loop
		\State \Return $tour$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

This greedy strategy ensures that at each step, the tour grows by adding the closest possible destination, optimizing for the immediate next step without regard for the overall tour length. While this method does not guarantee the discovery of the shortest possible tour, it significantly reduces the computation time compared to exhaustive search methods.

% TODO: Rearrange this

\subsubsection{Advantages and Limitations}

\textbf{Advantages:}
\begin{itemize}
	\item \textit{Simplicity}: Greedy algorithms are straightforward to implement and understand.
	\item \textit{Efficiency}: They often provide solutions quickly, making them suitable for problems where speed is crucial.
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
	\item \textit{Optimality}: Greedy choices do not always lead to the optimal solution, especially for complex problems like TSP.
	\item \textit{Short-sightedness}: By focusing on local optimality, they might overlook better solutions that require initial non-optimal choices.
\end{itemize}

Greedy algorithms, with their inherent simplicity and efficiency, play a crucial role in heuristic problem-solving, especially in cases where obtaining an exact solution is computationally infeasible. Their application to TSP highlights the balance between computational efficiency and solution quality, a theme that resonates across the study of heuristic and metaheuristic algorithms.

\section{Local Search Algorithms}

Local search algorithms represent a class of heuristic methods designed to explore the solution space of an optimization problem by iteratively moving from a solution to a neighboring solution in the search space. These algorithms are particularly effective for problems like the TSP, where finding an optimal solution is computationally infeasible for large instances. Local search provides a mechanism to improve an initial solution through small, localized changes.

\subsection{2-opt and 3-opt Techniques}

The 2-opt and 3-opt techniques are specific local search strategies used to refine TSP solutions. They work by iteratively removing two or three edges from the tour and reconnecting the segments in a different order, aiming to reduce the total tour length with each operation.

\subsubsection{2-opt Technique}

The 2-opt algorithm systematically checks each pair of edges in the tour and determines whether swapping them would result in a shorter path. This process is repeated until no further improvements can be made.


\begin{center}
	\begin{tikzpicture}
		% Define nodes
		\node[draw=none, fill=none] (A) at (0,2) {A};
		\node[draw=none] (B) at (2,1.7) {B};
		\node[draw=none] (C) at (4,1.5) {C};
		\node[draw=none] (D) at (6,2) {D};

		\node[draw=none] (G) at (0,0) {G};
		\node[draw=none] (F) at (2,0.2) {F};
		\node[draw=none] (E) at (4,0.5) {E};

		% Connect nodes
		\draw (A) -- (B);
		\draw[red] (B) -- (E);
		\draw (E) -- (D);
		\draw (D) -- (C);
		\draw[red] (C) -- (F);
		\draw (F) -- (G);
		\draw (G) -- (A);

		% Define nodes
		\node[draw=none] (A2) at (10-2,2) {A};
		\node[draw=none] (B2) at (12-2,1.7) {B};
		\node[draw=none] (C2) at (14-2,1.5) {C};
		\node[draw=none] (D2) at (16-2,2) {D};

		\node[draw=none] (G2) at (10-2,0) {G};
		\node[draw=none] (F2) at (12-2,0.2) {F};
		\node[draw=none] (E2) at (14-2,0.5) {E};

		% Connect nodes
		\draw (A2) -- (B2);
		\draw[blue] (B2) -- (C2);
		\draw (E2) -- (D2);
		\draw (D2) -- (C2);
		\draw[blue] (E2) -- (F2);
		\draw (F2) -- (G2);
		\draw (G2) -- (A2);




	\end{tikzpicture}
\end{center}


\begin{algorithm}
	\caption{2-opt Technique for TSP}\label{alg:twoopt}
	\begin{algorithmic}[1]
		\Procedure{TwoOpt}{$tour, distances$}
		\State $improved \gets true$
		\While{$improved$}
		\State $improved \gets false$
		\For{$i \gets 1$ to length($tour$) - 1}
		\For{$j \gets i+1$ to length($tour$)}
		\If{\Call{TwoOptSwap}{$tour, i, j$} $<$ \Call{TourDistance}{$tour$}}
		\State $tour \gets$ \Call{TwoOptSwap}{$tour, i, j$}
		\State $improved \gets true$
		\EndIf
		\EndFor
		\EndFor
		\EndWhile
		\State \Return $tour$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


\subsubsection{3-opt Technique}

The 3-opt technique extends the idea of the 2-opt by considering three edges for reconnection. This allows for a broader range of rearrangements at each step, potentially leading to more significant improvements at the expense of increased computational complexity.

\paragraph{Pseudocode for 3-opt Technique} is conceptually similar to 2-opt but involves more conditions for the swaps, reflecting the increased complexity of considering three edges at a time.

\subsection{Simulated Annealing}

Simulated Annealing is a probabilistic technique for approximating the global optimum of a given function. It is particularly useful for large problems like the TSP, where it seeks to escape local optima by allowing worse solutions with a probability that decreases over time, mimicking the cooling process of annealing in metallurgy.

\subsubsection{Application to TSP}

For the TSP, simulated annealing starts with a random tour and iteratively improves it by swapping cities. Unlike deterministic local search methods, it occasionally accepts worse solutions, providing a chance to escape local minima. The "temperature" parameter controls the probability of accepting worse solutions, gradually decreasing over time.

\paragraph{Pseudocode for Simulated Annealing}

\begin{algorithm}
	\caption{Simulated Annealing for TSP}\label{alg:simulatedannealing}
	\begin{algorithmic}[1]
		\Procedure{SimulatedAnnealingTSP}{$cities, initialTemperature, coolingRate$}
		\State $currentSolution \gets$ generateInitialSolution($cities$)
		\State $currentTemperature \gets initialTemperature$
		\While{$currentTemperature > 1$}
		\State $newSolution \gets$ generateNeighbor($currentSolution$)
		\State $currentEnergy \gets$ \Call{TourDistance}{$currentSolution$}
		\State $neighborEnergy \gets$ \Call{TourDistance}{$newSolution$}
		\If{\Call{AcceptanceProbability}{$currentEnergy, neighborEnergy, currentTemperature$} $>$ random(0, 1)}
		\State $currentSolution \gets newSolution$
		\EndIf
		\State $currentTemperature \gets currentTemperature \times coolingRate$
		\EndWhile
		\State \Return $currentSolution$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Local search algorithms, including the 2-opt and 3-opt techniques and simulated annealing, offer powerful methods for improving TSP solutions. By iteratively refining an initial solution, they balance between exploring the solution space and exploiting known good configurations, providing effective means of approaching optimal solutions for complex optimization problems.
\section{Metaheuristic Algorithms}

Metaheuristic algorithms are high-level strategies designed to navigate the search space of complex optimization problems efficiently. These algorithms do not guarantee an optimal solution but are effective for finding very good solutions within a reasonable timeframe, especially for problems where traditional methods falter due to the curse of dimensionality or the presence of numerous local optima.

\subsection{Genetic Algorithms}

Genetic Algorithms (GAs) are inspired by the process of natural selection and concepts of genetics such as mutation, crossover, and selection. In the context of TSP, a genetic algorithm evolves a population of tours (solutions) through successive generations, with the aim of improving the overall fitness of the population, measured as the inverse of the tour length.

\paragraph{Application to TSP}

For TSP, an initial population of random tours is first generated. The algorithm then applies crossover and mutation operators to breed new tours, selectively advancing tours with shorter paths to the next generation. Over time, the population converges towards better solutions.

\begin{algorithm}
	\caption{Genetic Algorithm for TSP}\label{alg:geneticalgorithm}
	\begin{algorithmic}[1]
		\Procedure{GeneticAlgorithmTSP}{$cities, populationSize, generations$}
		\State $population \gets$ generateInitialPopulation($cities, populationSize$)
		\For{$g \gets 1$ to $generations$}
		\State $newPopulation \gets$ select($population$)
		\State $newPopulation \gets$ crossover($newPopulation$)
		\State $newPopulation \gets$ mutate($newPopulation$)
		\State $population \gets$ $newPopulation$
		\EndFor
		\State $bestTour \gets$ getFittest($population$)
		\State \Return $bestTour$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Lin–Kernighan}
% TODO

\subsection{Tabu Search}

Tabu Search enhances local search by using memory structures that describe the visited solutions or user-defined conditions to avoid cycles. This approach allows the algorithm to escape local optima and explore the solution space more thoroughly.

\paragraph{Application to TSP}

In the TSP, Tabu Search iteratively improves a single solution by exploring its neighborhood, avoiding recently visited solutions or moves marked as "tabu" to prevent cycling back to less optimal solutions.

\begin{algorithm}
	\caption{Tabu Search for TSP}\label{alg:tabusearch}
	% \begin{algorithmic}[1]
	% Pseudocode placeholder for Tabu Search
	% \end{algorithmic}
\end{algorithm}

\subsection{Ant Colony Optimization}

Ant Colony Optimization (ACO) simulates the foraging behavior of ants to find the shortest paths between food sources and their nest. ACO has been effectively applied to TSP by simulating a colony of ants exploring various tours and gradually reinforcing the shorter paths with pheromones.

\paragraph{Application to TSP}

In TSP, ants construct tours based on the pheromone trails and the distance to the next city. Over successive iterations, paths forming part of shorter tours receive more pheromones, guiding future ants towards these promising paths.

\begin{algorithm}
	\caption{Ant Colony Optimization for TSP}\label{alg:antcolonyoptimization}
	% \begin{algorithmic}[1]
	% Pseudocode placeholder for Ant Colony Optimization
	% \end{algorithmic}
\end{algorithm}

Metaheuristic algorithms, including Genetic Algorithms, Tabu Search, and Ant Colony Optimization, offer powerful frameworks for tackling NP-hard problems like the TSP. By exploiting mechanisms inspired by natural processes and intelligent search strategies, these algorithms navigate the complex solution space of TSP to uncover high-quality solutions, balancing exploration and exploitation in the quest for near-optimal tours.
